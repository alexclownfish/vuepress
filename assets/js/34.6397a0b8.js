(window.webpackJsonp=window.webpackJsonp||[]).push([[34],{561:function(e,n,a){"use strict";a.r(n);var t=a(8),s=Object(t.a)({},(function(){var e=this,n=e.$createElement,a=e._self._c||n;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"prometheus-grafana-alertmanager监控k8s无坑版"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#prometheus-grafana-alertmanager监控k8s无坑版"}},[e._v("#")]),e._v(" prometheus+grafana+alertmanager监控k8s无坑版")]),e._v(" "),a("h2",{attrs:{id:"摘要"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#摘要"}},[e._v("#")]),e._v(" 摘要")]),e._v(" "),a("p",[e._v("k8s搭建完成并正常使用的基础上，需要有一个动态存储\n我的环境：")]),e._v(" "),a("table",[a("thead",[a("tr",[a("th",[e._v("k8s版本")]),e._v(" "),a("th",[e._v("Kubeadm部署 v1.18.0")])])]),e._v(" "),a("tbody",[a("tr",[a("td",[e._v("k8s-master")]),e._v(" "),a("td",[e._v("172.22.254.57")])]),e._v(" "),a("tr",[a("td",[e._v("k8s-node1")]),e._v(" "),a("td",[e._v("172.22.254.62")])]),e._v(" "),a("tr",[a("td",[e._v("k8s-node2")]),e._v(" "),a("td",[e._v("172.22.254.63(nfs服务端)")])]),e._v(" "),a("tr",[a("td",[e._v("StorageClass")]),e._v(" "),a("td",[e._v("nfs-storage")])])])]),e._v(" "),a("p",[e._v("k8s-master有污点，如果需要监控到master，去除污点即可（非必要）")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("kubectl taint nodes node1 key1=value1:NoSchedule-\n")])])]),a("p",[e._v("prometheus-rules中的规则字段可能随着版本更新出现变化，如有变化可以通知我，我实时更新文档。目前规则内的字段在此版本我已更新过。放心使用")]),e._v(" "),a("p",[e._v("还有一个小细节：prmetheus跟alertmanager的configmap是支持热更新的。可以用以下命令来热更新，可能执行刷新的时候会有点儿久，等一下就好")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("curl -X POST http://ClusterIP:PORT/-/reload\n")])])]),a("p",[e._v("资源下载："),a("a",{attrs:{href:"https://github.com/alexclownfish/k8s-monitor",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/alexclownfish/k8s-monitor"),a("OutboundLink")],1)]),e._v(" "),a("h2",{attrs:{id:"部署正文"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#部署正文"}},[e._v("#")]),e._v(" 部署正文")]),e._v(" "),a("h3",{attrs:{id:"创建ops命名空间"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#创建ops命名空间"}},[e._v("#")]),e._v(" 创建ops命名空间")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("kubectl create ns ops\n")])])]),a("h3",{attrs:{id:"prometheus-yaml文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#prometheus-yaml文件"}},[e._v("#")]),e._v(" prometheus yaml文件")]),e._v(" "),a("h4",{attrs:{id:"prometheus配置文件-prometheus-configmap-yaml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#prometheus配置文件-prometheus-configmap-yaml"}},[e._v("#")]),e._v(" prometheus配置文件 prometheus-configmap.yaml")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\n  namespace: ops \ndata:\n  prometheus.yml: |\n    rule_files:\n    - /etc/config/rules/*.rules\n\n    scrape_configs:\n    - job_name: prometheus\n      static_configs:\n      - targets:\n        - localhost:9090\n\n    - job_name: kubernetes-apiservers\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - action: keep\n        regex: default;kubernetes;https\n        source_labels:\n        - __meta_kubernetes_namespace\n        - __meta_kubernetes_service_name\n        - __meta_kubernetes_endpoint_port_name\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        insecure_skip_verify: true\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n \n    - job_name: kubernetes-nodes-kubelet\n      kubernetes_sd_configs:\n      - role: node  # 发现集群中的节点\n      relabel_configs:\n      # 将标签(.*)作为新标签名，原有值不变\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        insecure_skip_verify: true\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n    - job_name: kubernetes-nodes-cadvisor\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      # 将标签(.*)作为新标签名，原有值不变\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      # 实际访问指标接口 https://NodeIP:10250/metrics/cadvisor，这里替换默认指标URL路径\n      - target_label: __metrics_path__\n        replacement: /metrics/cadvisor\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        insecure_skip_verify: true\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n    - job_name: kubernetes-service-endpoints\n      kubernetes_sd_configs:\n      - role: endpoints  # 从Service列表中的Endpoint发现Pod为目标\n      relabel_configs:\n      # Service没配置注解prometheus.io/scrape的不采集\n      - action: keep\n        regex: true\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_scrape\n      # 重命名采集目标协议\n      - action: replace\n        regex: (https?)\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_scheme\n        target_label: __scheme__\n      # 重命名采集目标指标URL路径\n      - action: replace\n        regex: (.+)\n        source_labels:\n        - __meta_kubernetes_service_annotation_prometheus_io_path\n        target_label: __metrics_path__\n      # 重命名采集目标地址\n      - action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        source_labels:\n        - __address__\n        - __meta_kubernetes_service_annotation_prometheus_io_port\n        target_label: __address__\n      # 将K8s标签(.*)作为新标签名，原有值不变\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      # 生成命名空间标签\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: kubernetes_namespace\n      # 生成Service名称标签\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_service_name\n        target_label: kubernetes_name\n\n    - job_name: kubernetes-pods\n      kubernetes_sd_configs:\n      - role: pod   # 发现所有Pod为目标\n      # 重命名采集目标协议\n      relabel_configs:\n      - action: keep\n        regex: true\n        source_labels:\n        - __meta_kubernetes_pod_annotation_prometheus_io_scrape\n      # 重命名采集目标指标URL路径\n      - action: replace\n        regex: (.+)\n        source_labels:\n        - __meta_kubernetes_pod_annotation_prometheus_io_path\n        target_label: __metrics_path__\n      # 重命名采集目标地址\n      - action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        source_labels:\n        - __address__\n        - __meta_kubernetes_pod_annotation_prometheus_io_port\n        target_label: __address__\n      # 将K8s标签(.*)作为新标签名，原有值不变\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      # 生成命名空间标签\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_namespace\n        target_label: kubernetes_namespace\n      # 生成Service名称标签\n      - action: replace\n        source_labels:\n        - __meta_kubernetes_pod_name\n        target_label: kubernetes_pod_name\n\n    alerting:\n      alertmanagers:\n      - static_configs:\n          - targets: ["alertmanager:80"]\n')])])]),a("h4",{attrs:{id:"kube-state-metrics-采集了k8s中各种资源对象的状态信息-kube-state-metrics-yaml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kube-state-metrics-采集了k8s中各种资源对象的状态信息-kube-state-metrics-yaml"}},[e._v("#")]),e._v(" kube-state-metrics 采集了k8s中各种资源对象的状态信息 kube-state-metrics.yaml")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('apiVersion: apps/v1 \nkind: Deployment\nmetadata:\n  name: kube-state-metrics\n  namespace: ops\n  labels:\n    k8s-app: kube-state-metrics\nspec:\n  selector:\n    matchLabels:\n      k8s-app: kube-state-metrics\n      version: v1.3.0\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-state-metrics\n        version: v1.3.0\n    spec:\n      serviceAccountName: kube-state-metrics\n      containers:\n      - name: kube-state-metrics\n        image: lizhenliang/kube-state-metrics:v1.8.0 \n        ports:\n        - name: http-metrics\n          containerPort: 8080\n        - name: telemetry\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n      - name: addon-resizer\n        image: lizhenliang/addon-resizer:1.8.6\n        resources:\n          limits:\n            cpu: 100m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 30Mi\n        env:\n          - name: MY_POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: MY_POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n        volumeMounts:\n          - name: config-volume\n            mountPath: /etc/config\n        command:\n          - /pod_nanny\n          - --config-dir=/etc/config\n          - --container=kube-state-metrics\n          - --cpu=100m\n          - --extra-cpu=1m\n          - --memory=100Mi\n          - --extra-memory=2Mi\n          - --threshold=5\n          - --deployment=kube-state-metrics\n      volumes:\n        - name: config-volume\n          configMap:\n            name: kube-state-metrics-config\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-state-metrics-config\n  namespace: ops\ndata:\n  NannyConfiguration: |-\n    apiVersion: nannyconfig/v1alpha1\n    kind: NannyConfiguration\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-state-metrics\n  namespace: ops\n  annotations:\n    prometheus.io/scrape: \'true\'\nspec:\n  ports:\n  - name: http-metrics\n    port: 8080\n    targetPort: http-metrics\n    protocol: TCP\n  - name: telemetry\n    port: 8081\n    targetPort: telemetry\n    protocol: TCP\n  selector:\n    k8s-app: kube-state-metrics\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: kube-state-metrics\n  namespace: ops\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: kube-state-metrics\nrules:\n- apiGroups: [""]\n  resources:\n  - configmaps\n  - secrets\n  - nodes\n  - pods\n  - services\n  - resourcequotas\n  - replicationcontrollers\n  - limitranges\n  - persistentvolumeclaims\n  - persistentvolumes\n  - namespaces\n  - endpoints\n  verbs: ["list", "watch"]\n- apiGroups: ["apps"]\n  resources:\n  - statefulsets\n  - daemonsets\n  - deployments\n  - replicasets\n  verbs: ["list", "watch"]\n- apiGroups: ["batch"]\n  resources:\n  - cronjobs\n  - jobs\n  verbs: ["list", "watch"]\n- apiGroups: ["autoscaling"]\n  resources:\n  - horizontalpodautoscalers\n  verbs: ["list", "watch"]\n- apiGroups: ["networking.k8s.io", "extensions"]\n  resources:\n  - ingresses \n  verbs: ["list", "watch"]\n- apiGroups: ["storage.k8s.io"]\n  resources:\n  - storageclasses \n  verbs: ["list", "watch"]\n- apiGroups: ["certificates.k8s.io"]\n  resources:\n  - certificatesigningrequests\n  verbs: ["list", "watch"]\n- apiGroups: ["policy"]\n  resources:\n  - poddisruptionbudgets \n  verbs: ["list", "watch"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: kube-state-metrics-resizer\n  namespace: ops\nrules:\n- apiGroups: [""]\n  resources:\n  - pods\n  verbs: ["get"]\n- apiGroups: ["extensions","apps"]\n  resources:\n  - deployments\n  resourceNames: ["kube-state-metrics"]\n  verbs: ["get", "update"]\n---\napiVersion: rbac.authorization.k8s.io/v1 \nkind: ClusterRoleBinding\nmetadata:\n  name: kube-state-metrics\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kube-state-metrics\nsubjects:\n- kind: ServiceAccount\n  name: kube-state-metrics\n  namespace: ops\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kube-state-metrics\n  namespace: ops\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: kube-state-metrics-resizer\nsubjects:\n- kind: ServiceAccount\n  name: kube-state-metrics\n  namespace: ops\n')])])]),a("h4",{attrs:{id:"prometheus部署文件-prometheus-deploy-yaml-注意版本需要用2-20"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#prometheus部署文件-prometheus-deploy-yaml-注意版本需要用2-20"}},[e._v("#")]),e._v(" prometheus部署文件  prometheus-deploy.yaml(注意版本需要用2.20)")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus \n  namespace: ops\n  labels:\n    k8s-app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: prometheus\n  template:\n    metadata:\n      labels:\n        k8s-app: prometheus\n    spec:\n      serviceAccountName: prometheus\n      initContainers:\n      - name: "init-chown-data"\n        image: "busybox:latest"\n        imagePullPolicy: "IfNotPresent"\n        command: ["chown", "-R", "65534:65534", "/data"]\n        volumeMounts:\n        - name: prometheus-data\n          mountPath: /data\n          subPath: ""\n      containers:\n        - name: prometheus-server-configmap-reload\n          image: "jimmidyson/configmap-reload:v0.1"\n          imagePullPolicy: "IfNotPresent"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://localhost:9090/-/reload\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n            - mountPath: /etc/localtime\n              name: timezone\n          resources:\n            limits:\n              cpu: 10m\n              memory: 100Mi\n            requests:\n              cpu: 10m\n              memory: 100Mi\n\n        - name: prometheus-server\n          image: "prom/prometheus:v2.20.0"\n          imagePullPolicy: "IfNotPresent"\n          args:\n            - --config.file=/etc/config/prometheus.yml\n            - --storage.tsdb.path=/data\n            - --web.console.libraries=/etc/prometheus/console_libraries\n            - --web.console.templates=/etc/prometheus/consoles\n            - --web.enable-lifecycle\n          ports:\n            - containerPort: 9090\n          readinessProbe:\n            httpGet:\n              path: /-/ready\n              port: 9090\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          livenessProbe:\n            httpGet:\n              path: /-/healthy\n              port: 9090\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          resources:\n            limits:\n              cpu: 500m\n              memory: 800Mi\n            requests:\n              cpu: 200m\n              memory: 400Mi\n            \n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n            - name: prometheus-data\n              mountPath: /data\n              subPath: ""\n            - name: prometheus-rules\n              mountPath: /etc/config/rules\n            - mountPath: /etc/localtime\n              name: timezone  \n      volumes:\n        - name: config-volume\n          configMap:\n            name: prometheus-config\n        - name: prometheus-rules\n          configMap:\n            name: prometheus-rules\n        - name: prometheus-data\n          persistentVolumeClaim:\n            claimName: prometheus\n        - name: timezone\n          hostPath:\n            path: /usr/share/zoneinfo/Asia/Shanghai\n                                                  \n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: prometheus\n  namespace: ops\nspec:\n  storageClassName: "nfs-storage"\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n---\napiVersion: v1\nkind: Service\nmetadata: \n  name: prometheus\n  namespace: ops\nspec: \n  type: NodePort\n  ports: \n    - name: http \n      port: 9090\n      protocol: TCP\n      targetPort: 9090\n      nodePort: 30089\n  selector: \n    k8s-app: prometheus\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: ops\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n  - apiGroups:\n      - ""\n    resources:\n      - nodes\n      - nodes/metrics\n      - services\n      - endpoints\n      - pods\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - ""\n    resources:\n      - configmaps\n    verbs:\n      - get\n  - nonResourceURLs:\n      - "/metrics"\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: ops  \n')])])]),a("h4",{attrs:{id:"prometheus配置报警规则-prometheus-rules-yaml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#prometheus配置报警规则-prometheus-rules-yaml"}},[e._v("#")]),e._v(" prometheus配置报警规则 prometheus-rules.yaml")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-rules\n  namespace: ops\ndata:\n  general.rules: |\n    groups:\n    - name: general.rules\n      rules:\n      - alert: InstanceDown\n        expr: up == 0\n        for: 1m\n        labels:\n          severity: error \n        annotations:\n          summary: "Instance {{ $labels.instance }} 停止工作"\n          description: "{{ $labels.instance }} job {{ $labels.job }} 已经停止5分钟以上."\n               \n  node.rules: |\n    groups:\n    - name: node.rules\n      rules:\n      - alert: NodeFilesystemUsage\n        expr: |\n          100 - (node_filesystem_free_bytes / node_filesystem_size_bytes) * 100 > 60\n        for: 1m\n        labels:\n          severity: warning \n        annotations:\n          summary: "Instance {{ $labels.instance }} : {{ $labels.mountpoint }} 分区使用率过高"\n          description: "{{ $labels.instance }}: {{ $labels.mountpoint }} 分区使用大于60% (当前值: {{ $value }})"\n\n      - alert: NodeMemoryUsage\n        expr: |\n          100 - (node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes) / node_memory_MemTotal_bytes * 100 > 60\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "Instance {{ $labels.instance }} 内存使用率过高"\n          description: "{{ $labels.instance }}内存使用大于60% (当前值: {{ $value }})"\n\n      - alert: NodeCPUUsage    \n        expr: |\n          100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance) * 100) > 60 \n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "Instance {{ $labels.instance }} CPU使用率过高"       \n          description: "{{ $labels.instance }}CPU使用大于60% (当前值: {{ $value }})"\n\n      - alert: KubeNodeNotReady\n        expr: |\n          kube_node_status_condition{condition="Ready",status="true"} == 0\n        for: 1m\n        labels:\n          severity: error\n        annotations:\n          message: \'{{ $labels.node }} 已经有10多分钟没有准备好了.\'\n\n  pod.rules: |\n    groups:\n    - name: pod.rules\n      rules:\n      - alert: PodCPUUsage\n        expr: |\n           sum by(pod, namespace) (rate(container_cpu_usage_seconds_total{image!=""}[5m]) * 100) > 5\n        for: 5m\n        labels:\n          severity: warning \n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }} CPU使用大于80% (当前值: {{ $value }})"\n\n      - alert: PodMemoryUsage\n        expr: |\n           sum(container_memory_rss{image!=""}) by(pod, namespace) / sum(container_spec_memory_limit_bytes{image!=""}) by(pod, namespace) * 100 != +inf > 80\n        for: 5m\n        labels:\n          severity: error \n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }} 内存使用大于80% (当前值: {{ $value }})"\n\n      - alert: PodNetworkReceive\n        expr: |\n           sum(rate(container_network_receive_bytes_total{image!="",name=~"^k8s_.*"}[5m]) /1000) by (pod,namespace) > 30000\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }} 入口流量大于30MB/s (当前值: {{ $value }}K/s)"           \n\n      - alert: PodNetworkTransmit\n        expr: | \n           sum(rate(container_network_transmit_bytes_total{image!="",name=~"^k8s_.*"}[5m]) /1000) by (pod,namespace) > 30000\n        for: 5m\n        labels:\n          severity: warning \n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }} 出口流量大于30MB/s (当前值: {{ $value }}/K/s)"\n\n      - alert: PodRestart\n        expr: |\n           sum(changes(kube_pod_container_status_restarts_total[1m])) by (pod,namespace) > 0\n        for: 1m\n        labels:\n          severity: warning \n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }} Pod重启 (当前值: {{ $value }})"\n\n      - alert: PodFailed\n        expr: |\n           sum(kube_pod_status_phase{phase="Failed"}) by (pod,namespace) > 0\n        for: 5s\n        labels:\n          severity: error \n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }} Pod状态Failed (当前值: {{ $value }})"\n\n      - alert: PodPending\n        expr: | \n           sum(kube_pod_status_phase{phase="Pending"}) by (pod,namespace) > 0\n        for: 1m\n        labels:\n          severity: error\n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }} Pod状态Pending (当前值: {{ $value }})"\n\n      - alert: PodErrImagePull\n        expr: |\n           sum by(namespace,pod) (kube_pod_container_status_waiting_reason{reason="ErrImagePull"}) == 1\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }}  Pod状态ErrImagePull (当前值: {{ $value }})"\n\n      - alert: PodImagePullBackOff\n        expr: |\n           sum by(namespace,pod) (kube_pod_container_status_waiting_reason{reason="ImagePullBackOff"}) == 1\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }}  Pod状态ImagePullBackOff (当前值: {{ $value }})"\n\n      - alert: PodCrashLoopBackOff\n        expr: |\n           sum by(namespace,pod) (kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"}) == 1\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }}  Pod状态CrashLoopBackOff (当前值: {{ $value }})"\n\n      - alert: PodInvalidImageName\n        expr: |\n           sum by(namespace,pod) (kube_pod_container_status_waiting_reason{reason="InvalidImageName"}) == 1\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }}  Pod状态InvalidImageName (当前值: {{ $value }})"\n\n      - alert: PodCreateContainerConfigError\n        expr: |\n           sum by(namespace,pod) (kube_pod_container_status_waiting_reason{reason="CreateContainerConfigError"}) == 1\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: "命名空间: {{ $labels.namespace }} | Pod名称: {{ $labels.pod }}  Pod状态CreateContainerConfigError (当前值: {{ $value }})"\n\n  volume.rules: |\n    groups:\n    - name: volume.rules\n      rules:\n      - alert: PersistentVolumeClaimLost\n        expr: |\n           sum by(namespace, persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase="Lost"}) == 1\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is lost\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n      - alert: PersistentVolumeClaimPendig\n        expr: |\n           sum by(namespace, persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase="Pendig"}) == 1\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pendig\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n      - alert: PersistentVolume Failed\n        expr: |\n           sum(kube_persistentvolume_status_phase{phase="Failed",job="kubernetes-service-endpoints"}) by (persistentvolume) == 1\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: "Persistent volume is failed state\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n      - alert: PersistentVolume Pending\n        expr: |\n           sum(kube_persistentvolume_status_phase{phase="Pending",job="kubernetes-service-endpoints"}) by (persistentvolume) == 1\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: "Persistent volume is pending state\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}"\n\n')])])]),a("h4",{attrs:{id:"node-exporter配置node-exporter-yaml-注意版本需要用1-0-1"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#node-exporter配置node-exporter-yaml-注意版本需要用1-0-1"}},[e._v("#")]),e._v(" node-exporter配置node-exporter.yaml(注意版本需要用1.0.1)")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('apiVersion: apps/v1 \nkind: DaemonSet\nmetadata:\n  name: node-exporter \n  namespace: ops \n  labels:\n    k8s-app: node-exporter \nspec:\n  selector:\n    matchLabels:\n      k8s-app: node-exporter\n      version: v1.0.1\n  template:\n    metadata:\n      labels:\n        k8s-app: node-exporter \n        version: v1.0.1\n    spec:\n      containers:\n        - name: prometheus-node-exporter\n          image: "prom/node-exporter:v1.0.1"\n          #imagePullPolicy: "Always"\n          args:\n            - --path.procfs=/host/proc\n            - --path.sysfs=/host/sys\n          ports:\n            - name: metrics\n              containerPort: 9100\n              hostPort: 9100\n          volumeMounts:\n            - name: proc\n              mountPath: /host/proc\n              readOnly:  true\n            - name: sys\n              mountPath: /host/sys\n              readOnly: true\n          resources:\n            limits:\n              cpu: 10m\n              memory: 50Mi\n            requests:\n              cpu: 10m\n              memory: 50Mi\n      hostNetwork: true\n      hostPID: true\n      hostIPC: true\n      volumes:\n        - name: proc\n          hostPath:\n            path: /proc\n        - name: sys\n          hostPath:\n            path: /sys\n        - name: rootfs\n          hostPath:\n            path: /\n        - name: dev\n          hostPath:\n            path: /dev\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: node-exporter\n  namespace: ops\n  annotations:\n    prometheus.io/scrape: "true"\nspec:\n  clusterIP: None\n  ports:\n    - name: metrics\n      port: 9100\n      protocol: TCP\n      targetPort: 9100\n  selector:\n    k8s-app: node-exporter\n\n')])])]),a("h3",{attrs:{id:"alertmanager-yaml文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#alertmanager-yaml文件"}},[e._v("#")]),e._v(" alertmanager yaml文件")]),e._v(" "),a("h4",{attrs:{id:"alertmanager配置文件alertmanger-configmap-yaml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#alertmanager配置文件alertmanger-configmap-yaml"}},[e._v("#")]),e._v(" alertmanager配置文件alertmanger-configmap.yaml")]),e._v(" "),a("p",[e._v("注:邮箱需要自己去网易邮箱申请并且取得授权管理密码")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: alertmanager-config\n  namespace: ops\ndata:\n  alertmanager.yml: |-\n    global:\n      # 在没有报警的情况下声明为已解决的时间\n      resolve_timeout: 5m\n      # 配置邮件发送信息\n      smtp_smarthost: 'smtp.163.com:465'\n      smtp_from: 'xxx@163.com'\n      smtp_auth_username: 'xxx@163.com'\n      smtp_auth_password: 'xxxxxx'\n      smtp_hello: '163.com'\n      smtp_require_tls: false\n    # 所有报警信息进入后的根路由，用来设置报警的分发策略\n    route:\n      # 这里的标签列表是接收到报警信息后的重新分组标签，例如，接收到的报警信息里面有许多具有 cluster=A 和 alertname=LatncyHigh 这样的标签的报警信息将会批量被聚合到一个分组里面\n      group_by: ['alertname', 'cluster']\n      # 当一个新的报警分组被创建后，需要等待至少group_wait时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。\n      group_wait: 30s\n \n      # 当第一个报警发送后，等待'group_interval'时间来发送新的一组报警信息。\n      group_interval: 5m\n \n      # 如果一个报警信息已经发送成功了，等待'repeat_interval'时间来重新发送他们\n      repeat_interval: 5m\n \n      # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器\n      receiver: default\n \n      # 上面所有的属性都由所有子路由继承，并且可以在每个子路由上进行覆盖。\n      routes:\n      - receiver: email\n        group_wait: 10s\n        match:\n          team: node\n    templates:\n      - '/etc/config/template/email.tmpl'\n    receivers:\n    - name: 'default'\n      email_configs:\n      - to: 'xxxx@qq.com'\n        html: '{{ template \"email.html\" . }}'\n        headers: { Subject: \"[WARN] Prometheus 告警邮件\" }\n        #send_resolved: true\n    - name: 'email'\n      email_configs:\n      - to: 'xxxx@gmail.com'\n        send_resolved: true\n\n")])])]),a("h4",{attrs:{id:"alertmanager-template文件alertmanager-template-yaml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#alertmanager-template文件alertmanager-template-yaml"}},[e._v("#")]),e._v(" alertmanager template文件alertmanager-template.yaml")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('#自定义告警模板\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: alertmanager-template-volume\n  namespace: ops\ndata:\n  email.tmpl: |\n    {{ define "email.html" }}\n        {{ range .Alerts }}\n    <pre>\n        ========start==========\n       告警程序: prometheus_alert_email \n       告警级别: {{ .Labels.severity }} 级别 \n       告警类型: {{ .Labels.alertname }} \n       故障主机: {{ .Labels.instance }} \n       告警主题: {{ .Annotations.summary }}\n       告警详情: {{ .Annotations.description }}\n       处理方法: {{ .Annotations.console }}\n       触发时间: {{ .StartsAt.Format "2006-01-02 15:04:05" }}\n       ========end==========\n    </pre>\n        {{ end }}\n    {{ end }}\n\n')])])]),a("h4",{attrs:{id:"alertmanager部署文件alertmanager-deployment-yaml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#alertmanager部署文件alertmanager-deployment-yaml"}},[e._v("#")]),e._v(" alertmanager部署文件alertmanager-deployment.yaml")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: alertmanager\n  namespace: ops\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: alertmanager\n      version: v0.14.0\n  template:\n    metadata:\n      labels:\n        k8s-app: alertmanager\n        version: v0.14.0\n    spec:\n      containers:\n        - name: prometheus-alertmanager\n          image: "prom/alertmanager:v0.14.0"\n          imagePullPolicy: "IfNotPresent"\n          args:\n            - --config.file=/etc/config/alertmanager.yml\n            - --storage.path=/data\n            - --web.external-url=/\n          ports:\n            - containerPort: 9093\n          readinessProbe:\n            httpGet:\n              path: /#/status\n              port: 9093\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n#自定义告警模板\n            - name: config-template-volume\n              mountPath: /etc/config/template\n            - name: storage-volume\n              mountPath: "/data"\n              subPath: ""\n            - mountPath: /etc/localtime\n              name: timezone\n          resources:\n            limits:\n              cpu: 10m\n              memory: 200Mi\n            requests:\n              cpu: 10m\n              memory: 100Mi\n        - name: prometheus-alertmanager-configmap-reload\n          image: "jimmidyson/configmap-reload:v0.1"\n          imagePullPolicy: "IfNotPresent"\n          args:\n            - --volume-dir=/etc/config\n            - --webhook-url=http://localhost:9093/-/reload\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n          resources:\n            limits:\n              cpu: 10m\n              memory: 200Mi\n            requests:\n              cpu: 10m\n              memory: 100Mi\n      volumes:\n        - name: config-volume\n          configMap:\n            name: alertmanager-config\n        - name: config-template-volume\n          configMap:\n            name: alertmanager-template-volume\n        - name: storage-volume\n          persistentVolumeClaim:\n            claimName: alertmanager\n        - name: timezone\n          hostPath:\n            path: /usr/share/zoneinfo/Asia/Shanghai\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: alertmanager\n  namespace: ops\nspec:\n  storageClassName: nfs-storage\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: "2Gi"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: alertmanager\n  namespace: ops\n  labels:\n    kubernetes.io/cluster-service: "true"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: "Alertmanager"\nspec:\n  type: "NodePort"\n  ports:\n    - name: http\n      port: 80\n      protocol: TCP\n      targetPort: 9093\n      nodePort: 30093\n  selector:\n    k8s-app: alertmanager\n\n\n')])])]),a("h3",{attrs:{id:"grafana-yaml文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#grafana-yaml文件"}},[e._v("#")]),e._v(" grafana yaml文件")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('apiVersion: apps/v1\nkind: Deployment \nmetadata:\n  name: grafana\n  namespace: ops\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:7.1.0\n        ports:\n          - containerPort: 3000\n            protocol: TCP\n        resources:\n          limits:\n            cpu: 100m            \n            memory: 256Mi          \n          requests:\n            cpu: 100m            \n            memory: 256Mi\n        volumeMounts:\n          - name: grafana-data\n            mountPath: /var/lib/grafana\n            subPath: grafana\n          - mountPath: /etc/localtime\n            name: timezone\n      securityContext:\n        fsGroup: 472\n        runAsUser: 472\n      volumes:\n      - name: grafana-data\n        persistentVolumeClaim:\n          claimName: grafana\n      - name: timezone\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai \n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: grafana \n  namespace: ops\nspec:\n  storageClassName: "nfs-storage"\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\n  namespace: ops\nspec:\n  type: NodePort\n  ports:\n  - port : 80\n    targetPort: 3000\n    nodePort: 30030\n  selector:\n    app: grafana\n\n')])])]),a("h3",{attrs:{id:"部署到k8s中"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#部署到k8s中"}},[e._v("#")]),e._v(" 部署到k8s中")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("kubectl apply -f .\n")])])]),a("h2",{attrs:{id:"grafana数据源和监控"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#grafana数据源和监控"}},[e._v("#")]),e._v(" grafana数据源和监控")]),e._v(" "),a("h3",{attrs:{id:"grafana添加数据源"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#grafana添加数据源"}},[e._v("#")]),e._v(" grafana添加数据源")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/img_convert/2e2b52257bf58eb3de3800b73bf1b33a.png",alt:"在这里插入图片描述"}}),e._v("\n点击datasource - add datasource\n"),a("img",{attrs:{src:"https://img-blog.csdnimg.cn/img_convert/f4a6d0482662d940b54f5345ff5719b6.png",alt:"在这里插入图片描述"}}),e._v("\n之后点击save&test,添加数据源结束")]),e._v(" "),a("h3",{attrs:{id:"import导入模板"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#import导入模板"}},[e._v("#")]),e._v(" import导入模板")]),e._v(" "),a("p",[e._v("模板下载："),a("a",{attrs:{href:"https://github.com/alexclownfish/k8s-monitor/tree/main/grafana_template",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/alexclownfish/k8s-monitor/tree/main/grafana_template"),a("OutboundLink")],1),e._v(" "),a("img",{attrs:{src:"https://img-blog.csdnimg.cn/img_convert/0da7161b13ec62e8a64bb700885473fc.png",alt:"在这里插入图片描述"}}),e._v(" "),a("img",{attrs:{src:"https://img-blog.csdnimg.cn/img_convert/6dc324132e15af124c96a6cb10632f20.png",alt:"在这里插入图片描述"}})]),e._v(" "),a("h3",{attrs:{id:"修改prometheus-rules验证监控触发报警并发送邮件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#修改prometheus-rules验证监控触发报警并发送邮件"}},[e._v("#")]),e._v(" 修改prometheus rules验证监控触发报警并发送邮件")]),e._v(" "),a("p",[e._v("修改prometheus-rules.yaml\n"),a("img",{attrs:{src:"https://img-blog.csdnimg.cn/img_convert/68786c7f9abf4518c5cf910056074369.png",alt:"在这里插入图片描述"}})]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("#热更新configmap\nkubectl apply -f prometheus-rules.yaml\ncurl -X POST http://10.1.230.219:9090/-/reload\n")])])]),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/img_convert/9bed866e7556554490febdfb84bf3769.png",alt:"在这里插入图片描述"}}),e._v(" "),a("img",{attrs:{src:"https://img-blog.csdnimg.cn/img_convert/7a5776cc75bbd830ee989699467317c4.png",alt:"在这里插入图片描述"}})]),e._v(" "),a("p",[e._v("看到已经触发报警并发送邮件\n至此结束")]),e._v(" "),a("h2",{attrs:{id:"感谢大佬"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#感谢大佬"}},[e._v("#")]),e._v(" 感谢大佬")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://blog.51cto.com/luoguoling",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://blog.51cto.com/luoguoling"),a("OutboundLink")],1),e._v(" "),a("a",{attrs:{href:"https://alexcld.com",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://alexcld.com"),a("OutboundLink")],1)])])}),[],!1,null,null,null);n.default=s.exports}}]);